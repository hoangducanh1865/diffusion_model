{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5999ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21c4148e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rand = torch.randn(4, 3, 64, 64)\\npred_noise = torch.randn_like(rand)\\nrandtime = torch.randint(0, 1000, (4,))\\nsampler.remove_noise(image=rand, timesteps=randtime, predicted_noise=pred_noise)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sampler:\n",
    "    def __init__(self, num_steps=1000, beta_start=0.0001, beta_end=0.02):\n",
    "        self.num_steps = num_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.beta_schedule = self.linear_beta_schedule()\n",
    "        self.alpha = 1 - self.beta_schedule\n",
    "        self.alpha_cummulative_prod = torch.cumprod(self.alpha, dim=-1)\n",
    "\n",
    "    def linear_beta_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.num_steps)\n",
    "\n",
    "    def _repeated_unsqueeze(self, target, tensor):\n",
    "        while target.dim() > tensor.dim():\n",
    "            tensor = tensor.unsqueeze(-1)\n",
    "        return tensor\n",
    "\n",
    "    def add_noise(self, image, timesteps):\n",
    "        batch_size, c, h, w = image.shape\n",
    "        device = image.device\n",
    "        alpha_cummulative_prod_timesteps = self.alpha_cummulative_prod[timesteps].to(\n",
    "            device\n",
    "        )\n",
    "        mean_coeff = alpha_cummulative_prod_timesteps**0.5\n",
    "        var_coeff = (1 - alpha_cummulative_prod_timesteps) ** 0.5\n",
    "        mean_coeff = self._repeated_unsqueeze(image, mean_coeff)\n",
    "        var_coeff = self._repeated_unsqueeze(image, var_coeff)\n",
    "        noise = torch.randn_like(image)\n",
    "        \"\"\"print(mean_coeff.shape)\n",
    "        print(image.shape)\"\"\"\n",
    "        noisy_image = mean_coeff * image + var_coeff * noise\n",
    "        return noisy_image, noise\n",
    "\n",
    "    def remove_noise(self, image, timesteps, predicted_noise):\n",
    "        b, c, h, w = image.shape\n",
    "        device = image.device\n",
    "        equal_to_zero_mask = timesteps == 0\n",
    "        beta_t = self.beta_schedule[timesteps].to(device)\n",
    "        alpha_t = self.alpha[timesteps].to(device)\n",
    "        alpha_cummulative_prod_t = self.alpha_cummulative_prod[timesteps].to(device)\n",
    "        alpha_cummulative_prod_t_prev = self.alpha_cummulative_prod[timesteps - 1].to(\n",
    "            device\n",
    "        )\n",
    "        alpha_cummulative_prod_t_prev[equal_to_zero_mask] = (\n",
    "            1.0  # @QUESTION: this line of code looks weird\n",
    "        )\n",
    "        noise = torch.randn_like(\n",
    "            image\n",
    "        )  # This is element z in line 4 in Algorithm 2 Sampling\n",
    "        variance = (\n",
    "            beta_t\n",
    "            * (1 - alpha_cummulative_prod_t_prev)\n",
    "            / (1 - alpha_cummulative_prod_t)\n",
    "        )  # This is element beta_t_hat in formula (7)\n",
    "        variance = self._repeated_unsqueeze(image, variance)\n",
    "        sigma_t_z = (\n",
    "            variance**0.5\n",
    "        ) * noise  # This is element sigma * z in line 4 in Algorithm 2 Sampling\n",
    "        noise_coff = (\n",
    "            beta_t / (1 - alpha_cummulative_prod_t) ** 0.5\n",
    "        )  # This is an element in line 4 in Algorithm 2 Sampling, in the paper, they write beta_t in form of (1 - alpha_t)\n",
    "        noise_coff = self._repeated_unsqueeze(image, noise_coff)\n",
    "        reciprocal_root_alpha_t = alpha_t ** (\n",
    "            -0.5\n",
    "        )  # This is the first element in Algorithm 2 Sampling\n",
    "        reciprocal_root_alpha_t = self._repeated_unsqueeze(\n",
    "            image, reciprocal_root_alpha_t\n",
    "        )\n",
    "\n",
    "        # Final formula in Algorithm 2 Sampling\n",
    "        mean = reciprocal_root_alpha_t * (image - noise_coff * predicted_noise)\n",
    "        denoised = mean + sigma_t_z\n",
    "\n",
    "        return denoised\n",
    "\n",
    "\n",
    "sampler = Sampler()\n",
    "\"\"\"rand = torch.randn(4, 3, 64, 64)\n",
    "pred_noise = torch.randn_like(rand)\n",
    "randtime = torch.randint(0, 1000, (4,))\n",
    "sampler.remove_noise(image=rand, timesteps=randtime, predicted_noise=pred_noise)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a1d745f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 14, 14])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels, num_heads=12, attn_p=0, proj_p=0):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = in_channels // num_heads\n",
    "        self.scale = self.head_dim ** (-0.5)  # 1 / sqrt(d)\n",
    "        self.query = nn.Linear(in_channels, in_channels)\n",
    "        self.key = nn.Linear(in_channels, in_channels)\n",
    "        self.value = nn.Linear(in_channels, in_channels)\n",
    "        self.attn_p = attn_p\n",
    "        self.proj = nn.Linear(in_channels, in_channels)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        q = (\n",
    "            self.query(x)\n",
    "            .reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        k = (\n",
    "            self.key(x)\n",
    "            .reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        v = (\n",
    "            self.value(x)\n",
    "            .reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        x = F.scaled_dot_product_attention(q, k, v, dropout_p=self.attn_p)\n",
    "        x = x.transpose(1, 2).reshape(batch_size, seq_len, embed_dim)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        \"\"\"print(x.shape)\"\"\"\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_channels, mlp_ratio=4, mlp_p=0):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(in_channels, in_channels * mlp_ratio)\n",
    "        self.act = nn.GELU()\n",
    "        self.drop_1 = nn.Dropout(mlp_p)\n",
    "        self.fc_2 = nn.Linear(in_channels * mlp_ratio, in_channels)\n",
    "        self.drop_2 = nn.Dropout(mlp_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop_1(x)\n",
    "        x = self.fc_2(x)\n",
    "        x = self.drop_2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, num_heads=4, mlp_ratio=2, proj_p=0, attn_p=0, mlp_p=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm_1 = nn.LayerNorm(\n",
    "            in_channels, eps=1e-6\n",
    "        )  # @QUESTION: what does eps mean?\n",
    "        self.attn = SelfAttention(\n",
    "            in_channels=in_channels, num_heads=num_heads, attn_p=attn_p, proj_p=proj_p\n",
    "        )\n",
    "        self.norm_2 = nn.LayerNorm(in_channels, eps=1e-6)\n",
    "        self.mlp = MLP(in_channels=in_channels, mlp_ratio=mlp_ratio, mlp_p=mlp_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape  # batch_size, channels, height, weight\n",
    "        x = x.reshape(b, c, h * w).permute(0, 2, 1)  # Swap dim 1 anf dim 2\n",
    "        x = x + self.attn(self.norm_1(x))\n",
    "        x = x + self.mlp(self.norm_2(x))\n",
    "        x = x.permute(0, 2, 1).reshape(b, c, h, w)\n",
    "        return x\n",
    "\n",
    "\n",
    "rand = torch.randn(4, 64, 14, 14)\n",
    "t = TransformerBlock(in_channels=64, num_heads=4)\n",
    "t(rand).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c0e52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6113e-02,  1.2789e-01, -5.5721e-02,  6.7660e-02, -5.3279e-02,\n",
       "         -4.4815e-02, -3.4111e-02,  1.7444e-01,  9.2044e-03, -1.3521e-03,\n",
       "         -1.2666e-01, -5.5095e-04,  9.1016e-03,  4.5437e-02,  7.0198e-03,\n",
       "          4.8164e-02,  2.7518e-02,  4.1621e-02, -1.2556e-01, -1.6496e-02,\n",
       "         -3.6184e-02,  1.0916e-02, -3.0518e-02, -1.0314e-01, -1.7973e-02,\n",
       "         -5.9165e-02,  3.6415e-02, -1.2168e-01,  5.0992e-02,  6.3842e-02,\n",
       "         -7.3416e-02,  6.2946e-03, -6.0360e-02, -1.1590e-01, -7.5424e-02,\n",
       "         -4.8699e-02,  6.4675e-02,  7.2463e-02,  3.0495e-03,  5.0382e-02,\n",
       "         -3.8634e-02, -9.8698e-02,  1.4548e-03,  2.8592e-02, -3.5729e-02,\n",
       "         -1.0180e-01,  1.2935e-01, -4.7959e-02,  1.2366e-01,  2.1530e-02,\n",
       "         -8.4555e-02,  2.2867e-02,  9.7375e-02,  1.5250e-01, -1.3843e-01,\n",
       "         -5.2180e-02,  5.4016e-02, -2.8075e-02,  8.4279e-04,  1.2376e-01,\n",
       "         -5.2753e-02,  6.4879e-02,  1.8049e-01, -1.5142e-02, -1.8976e-02,\n",
       "         -1.0289e-01,  3.9568e-02,  5.0806e-02, -2.4091e-02, -8.9422e-03,\n",
       "         -1.0683e-01, -1.0144e-01, -1.0139e-01, -6.4827e-02, -1.5937e-01,\n",
       "          7.1751e-02,  1.9465e-01,  5.6818e-02, -2.1392e-02, -1.0334e-01,\n",
       "         -2.3173e-02,  1.1648e-01,  3.1691e-02, -1.7312e-02,  6.0256e-03,\n",
       "         -5.1867e-02, -6.2395e-02,  1.9657e-03,  6.9919e-03, -8.5437e-02,\n",
       "         -9.4364e-02, -6.4527e-02,  3.2850e-02, -1.0745e-01, -1.0782e-01,\n",
       "          6.4613e-02, -9.2474e-02, -3.9331e-02, -9.9734e-02,  7.4616e-02,\n",
       "          2.0035e-02,  2.3442e-02,  7.3282e-02, -2.7248e-02,  1.2158e-01,\n",
       "         -1.6243e-02,  8.7009e-02, -8.5212e-02,  8.9038e-02,  2.0266e-01,\n",
       "          7.4186e-02,  5.8385e-02,  3.7438e-02,  2.1850e-03,  1.0545e-01,\n",
       "         -7.2353e-03, -1.8674e-02, -1.7618e-03,  8.3665e-03, -4.7252e-02,\n",
       "         -3.9928e-03, -5.0239e-02, -6.6144e-04, -1.1652e-01,  1.0120e-01,\n",
       "         -3.6370e-03, -1.8310e-02, -1.6646e-02,  1.8338e-01,  6.9768e-02,\n",
       "         -8.5455e-02, -3.2224e-02,  1.4095e-01, -8.2571e-02, -3.7833e-02,\n",
       "         -4.5635e-02,  5.0333e-02,  5.6855e-02, -4.7367e-02, -1.3889e-02,\n",
       "          9.6216e-03, -9.7923e-02, -7.3504e-02,  1.5347e-01,  1.2339e-01,\n",
       "         -1.4019e-02, -3.5004e-02, -5.7333e-02,  1.0996e-01,  5.4862e-03,\n",
       "         -4.6699e-02,  1.3134e-01, -1.7936e-02,  1.3326e-02, -6.8663e-02,\n",
       "          7.9643e-03, -1.6816e-03, -3.4881e-03, -5.3528e-02,  9.2968e-02,\n",
       "          3.9133e-03,  2.9036e-03, -1.6957e-02,  9.3330e-02, -9.5004e-02,\n",
       "         -3.5222e-03, -8.5115e-02, -5.6184e-02,  1.1483e-02,  1.4150e-02,\n",
       "          9.4013e-02,  4.5344e-02, -2.7492e-02, -6.0342e-02, -3.4187e-02,\n",
       "         -4.4504e-02, -1.5519e-02, -8.4099e-02, -7.7600e-02, -8.2235e-02,\n",
       "         -7.2407e-02,  2.0881e-01,  1.6446e-01, -1.2576e-01,  2.4718e-02,\n",
       "         -2.0803e-02, -5.3353e-02, -3.5283e-02,  4.6295e-02,  5.9729e-02,\n",
       "         -5.6421e-02,  3.0005e-03, -7.9623e-03, -8.9812e-02,  5.8290e-02,\n",
       "         -9.1177e-02, -1.5602e-02,  8.4668e-03, -6.2740e-02, -7.5567e-02,\n",
       "         -3.2495e-02, -4.8848e-02, -8.4558e-02,  9.8976e-02,  9.4266e-02,\n",
       "          4.5276e-02, -4.6504e-02,  6.7614e-02,  4.9612e-02,  1.0894e-01,\n",
       "          2.0601e-01,  3.9557e-02,  1.7581e-01,  8.1030e-02, -4.9736e-02,\n",
       "          1.9860e-02, -3.7299e-02,  1.2594e-01,  2.5924e-02,  1.4341e-01,\n",
       "         -1.8491e-02, -3.6220e-02,  4.5896e-06, -9.2775e-02,  1.9641e-03,\n",
       "          6.9943e-03, -7.7134e-02, -1.8565e-02, -2.8474e-02, -2.7247e-02,\n",
       "          3.3609e-04, -1.0391e-01,  1.2445e-03,  3.2557e-02, -4.8137e-02,\n",
       "          8.2464e-02,  1.0292e-01,  5.5347e-03, -5.6052e-02, -1.2158e-01,\n",
       "          3.6211e-02,  4.4322e-03, -5.0410e-02, -1.0869e-01,  7.9931e-03,\n",
       "          4.3492e-03,  5.9769e-02, -4.6321e-02,  2.6093e-02, -6.5289e-02,\n",
       "          5.7582e-02,  8.7420e-02, -1.8883e-02, -6.0111e-02, -2.8520e-02,\n",
       "         -6.8632e-02],\n",
       "        [-2.2643e-02,  1.0432e-01, -7.1105e-02,  2.5398e-02, -6.7197e-02,\n",
       "         -4.6162e-02, -4.1075e-02,  1.7962e-01,  1.9166e-02,  1.3311e-02,\n",
       "         -1.3199e-01, -6.7573e-03,  2.4708e-02,  4.2002e-02,  1.2870e-02,\n",
       "          3.2050e-02,  3.4909e-03,  7.3705e-02, -1.3609e-01, -1.5122e-02,\n",
       "         -4.5338e-02,  9.4848e-03, -2.5561e-02, -1.0738e-01, -2.6902e-03,\n",
       "         -5.2707e-02,  3.8387e-02, -1.1610e-01,  3.4319e-02,  6.0626e-02,\n",
       "         -6.7484e-02,  1.7665e-02, -5.7925e-02, -1.1645e-01, -8.2418e-02,\n",
       "         -6.2000e-02,  1.1864e-01,  7.1427e-02,  1.3030e-03,  5.7252e-02,\n",
       "         -4.7351e-02, -7.0208e-02,  9.8623e-03, -2.1772e-02, -4.1062e-02,\n",
       "         -1.0836e-01,  1.0941e-01, -2.5668e-02,  1.0631e-01,  5.0774e-02,\n",
       "         -9.8882e-02,  2.9804e-02,  7.6125e-02,  1.8198e-01, -1.4218e-01,\n",
       "         -2.3495e-02,  5.4904e-02, -1.5290e-02, -1.5575e-03,  1.2156e-01,\n",
       "         -4.1843e-02,  9.4102e-02,  1.7143e-01, -3.0394e-02, -1.6542e-02,\n",
       "         -1.0188e-01,  9.0565e-03,  4.4838e-02, -2.2135e-02, -2.3681e-02,\n",
       "         -1.1798e-01, -9.1957e-02, -9.0787e-02, -6.7004e-02, -1.6238e-01,\n",
       "          8.9739e-02,  1.8237e-01,  7.4594e-02, -1.9081e-03, -1.0717e-01,\n",
       "         -1.1438e-02,  1.2183e-01,  1.2863e-02, -1.9125e-02,  1.6555e-02,\n",
       "         -7.1574e-02, -4.1698e-02,  1.7548e-02, -5.2885e-03, -9.9950e-02,\n",
       "         -9.4971e-02, -5.7846e-02,  2.8885e-02, -1.0473e-01, -1.0262e-01,\n",
       "          5.7552e-02, -8.8454e-02, -2.6980e-02, -1.2051e-01,  3.1554e-02,\n",
       "          2.5850e-02,  1.2956e-02,  4.7089e-02, -4.0597e-02,  1.3170e-01,\n",
       "         -9.5785e-03,  7.9621e-02, -7.7971e-02,  7.4606e-02,  1.8574e-01,\n",
       "          5.0707e-02,  7.7671e-02,  6.2788e-02, -2.3013e-02,  1.1113e-01,\n",
       "          6.1196e-03, -3.6394e-02,  7.6920e-03,  2.6941e-02, -4.2710e-02,\n",
       "          2.3399e-02, -3.8935e-02,  1.4576e-02, -9.7762e-02,  4.6583e-02,\n",
       "          1.2091e-02, -2.7257e-02, -6.0251e-03,  1.6002e-01,  4.8032e-02,\n",
       "         -8.7609e-02, -1.0686e-02,  8.9390e-02, -1.0533e-01, -3.3083e-02,\n",
       "         -5.3690e-02,  2.8261e-02,  8.2407e-02, -4.3923e-02,  7.8056e-03,\n",
       "          1.8320e-02, -1.0212e-01, -5.6305e-02,  1.5206e-01,  1.3548e-01,\n",
       "         -4.3249e-02, -1.1460e-02, -5.5150e-02,  1.2616e-01, -2.4004e-02,\n",
       "         -3.7248e-02,  1.1828e-01,  5.2146e-04, -1.6454e-02, -7.0578e-02,\n",
       "          3.1069e-03, -2.9973e-02,  1.4629e-02, -4.1662e-02,  7.2105e-02,\n",
       "         -2.1936e-02,  4.0839e-02, -4.4992e-03,  1.0056e-01, -1.1308e-01,\n",
       "         -1.6665e-02, -7.9701e-02, -6.4182e-02, -2.4818e-02, -1.6343e-02,\n",
       "          1.1701e-01,  5.2389e-02, -4.0812e-02, -5.9411e-02, -1.7918e-02,\n",
       "         -6.8081e-02,  1.2776e-02, -5.2587e-02, -8.6948e-02, -9.8794e-02,\n",
       "         -7.3727e-02,  1.9972e-01,  1.8522e-01, -1.3394e-01,  2.6883e-02,\n",
       "         -2.9369e-03, -8.1765e-02, -4.5034e-02,  3.1237e-02,  2.8423e-02,\n",
       "         -2.5449e-02,  1.0408e-02, -2.2790e-02, -9.6862e-02,  5.7272e-02,\n",
       "         -8.7952e-02, -2.7871e-02,  3.0107e-02, -5.3088e-02, -5.6472e-02,\n",
       "         -5.2845e-02, -5.0614e-02, -8.4786e-02,  1.1513e-01,  1.0898e-01,\n",
       "          5.6554e-02, -4.3033e-02,  1.0014e-01,  5.3919e-02,  9.2334e-02,\n",
       "          1.8900e-01,  4.1152e-02,  1.6924e-01,  9.6142e-02, -2.7037e-02,\n",
       "          2.0985e-02, -4.4805e-02,  9.3966e-02,  2.1296e-02,  1.3210e-01,\n",
       "         -3.2557e-02, -5.1827e-02,  9.5425e-03, -9.0671e-02,  2.0099e-02,\n",
       "         -5.3894e-04, -6.6083e-02, -1.2007e-02, -4.5938e-02,  4.9625e-03,\n",
       "          8.7325e-03, -1.1836e-01,  1.5513e-02,  3.5897e-02, -6.2686e-02,\n",
       "          5.7073e-02,  7.0487e-02, -3.0607e-02, -4.7404e-02, -1.4049e-01,\n",
       "          2.7393e-02,  5.4399e-03, -5.5420e-02, -9.7775e-02,  1.9038e-02,\n",
       "          3.4159e-03,  2.6120e-02, -3.5141e-02,  2.1873e-02, -7.3443e-02,\n",
       "          8.5183e-02,  1.1999e-01, -1.3425e-02, -6.1136e-02, -2.1341e-02,\n",
       "         -6.2107e-02],\n",
       "        [-3.0384e-02,  6.1404e-02, -8.2890e-02, -5.4214e-03, -5.7646e-02,\n",
       "         -3.7373e-02, -4.2757e-02,  1.8002e-01,  3.6916e-02,  1.7317e-02,\n",
       "         -1.3385e-01, -9.6707e-03,  2.8252e-02,  3.2402e-02,  2.0261e-02,\n",
       "          2.2590e-02, -2.2167e-04,  1.0945e-01, -1.3356e-01, -1.5527e-02,\n",
       "         -3.1214e-02,  7.3046e-03, -3.0040e-02, -1.0048e-01,  9.3078e-03,\n",
       "         -4.8825e-02,  4.1967e-02, -1.0167e-01,  3.5316e-02,  5.2539e-02,\n",
       "         -6.6648e-02,  1.0332e-02, -4.0809e-02, -1.0540e-01, -8.6226e-02,\n",
       "         -7.5642e-02,  1.5159e-01,  8.5297e-02, -2.7724e-04,  6.8580e-02,\n",
       "         -4.1487e-02, -5.1864e-02,  2.0568e-02, -6.5766e-02, -4.1140e-02,\n",
       "         -1.2063e-01,  8.0295e-02, -2.2846e-03,  9.0183e-02,  6.6846e-02,\n",
       "         -1.0342e-01,  3.5421e-02,  4.7989e-02,  1.8066e-01, -1.5688e-01,\n",
       "          1.2007e-03,  5.1464e-02, -8.7935e-03,  1.0336e-02,  1.0100e-01,\n",
       "         -4.2495e-02,  1.1587e-01,  1.3647e-01, -4.2508e-02, -1.8406e-02,\n",
       "         -8.9679e-02, -2.8196e-03,  2.7081e-02, -2.2919e-02, -4.7547e-02,\n",
       "         -1.2206e-01, -8.4523e-02, -8.4055e-02, -6.0555e-02, -1.4613e-01,\n",
       "          9.6392e-02,  1.6988e-01,  7.0083e-02,  2.2108e-02, -1.0852e-01,\n",
       "          1.5462e-02,  1.2932e-01, -1.5017e-02, -1.7105e-02,  6.6656e-03,\n",
       "         -8.7187e-02, -1.7409e-02,  1.8660e-02, -3.3949e-03, -1.0836e-01,\n",
       "         -9.8658e-02, -4.5212e-02,  2.5865e-02, -9.4971e-02, -8.7006e-02,\n",
       "          4.5451e-02, -6.3921e-02, -7.4108e-03, -1.4973e-01, -1.3047e-02,\n",
       "          3.4232e-02,  2.5403e-03,  1.5386e-02, -4.1736e-02,  1.2532e-01,\n",
       "         -1.2438e-02,  8.0868e-02, -7.4697e-02,  4.9550e-02,  1.8007e-01,\n",
       "          3.5019e-02,  9.9388e-02,  9.0079e-02, -5.3744e-02,  1.2986e-01,\n",
       "          3.3109e-02, -4.4833e-02,  2.5423e-02,  4.4447e-02, -4.0077e-02,\n",
       "          5.5044e-02, -1.3184e-02,  2.6049e-02, -7.1566e-02,  5.4856e-03,\n",
       "          2.1121e-02, -2.3168e-02,  2.0529e-02,  1.2640e-01,  2.5881e-02,\n",
       "         -8.3843e-02,  7.0726e-03,  3.5904e-02, -1.2067e-01, -2.3847e-02,\n",
       "         -4.3919e-02,  3.4854e-04,  8.3349e-02, -4.3075e-02,  3.1013e-02,\n",
       "          3.5711e-02, -1.1408e-01, -3.7320e-02,  1.4327e-01,  1.3730e-01,\n",
       "         -5.7217e-02, -4.8130e-04, -4.5588e-02,  1.3770e-01, -5.4516e-02,\n",
       "         -3.5740e-02,  9.6333e-02,  2.6916e-02, -2.6768e-02, -6.7129e-02,\n",
       "          1.7373e-02, -5.5136e-02,  2.0565e-02, -2.4609e-02,  6.4935e-02,\n",
       "         -3.4699e-02,  6.3230e-02, -3.9993e-03,  9.9075e-02, -1.2656e-01,\n",
       "         -2.9520e-02, -8.2650e-02, -6.9483e-02, -5.6421e-02, -2.7825e-02,\n",
       "          1.4462e-01,  4.8123e-02, -6.5102e-02, -5.4150e-02, -3.1675e-03,\n",
       "         -9.1189e-02,  2.8961e-02, -3.0221e-02, -1.0467e-01, -1.0920e-01,\n",
       "         -6.5290e-02,  1.8739e-01,  1.8229e-01, -1.3858e-01, -1.0577e-02,\n",
       "          1.7602e-02, -1.0522e-01, -5.3355e-02,  3.6212e-03,  4.3654e-03,\n",
       "          1.0399e-02, -6.5048e-03, -3.5670e-02, -1.0715e-01,  6.1462e-02,\n",
       "         -7.4993e-02, -3.9228e-02,  5.2137e-02, -5.8107e-02, -2.6593e-02,\n",
       "         -6.7422e-02, -5.0809e-02, -6.7625e-02,  1.3224e-01,  1.1121e-01,\n",
       "          6.8341e-02, -4.8503e-02,  1.2074e-01,  5.6135e-02,  7.8501e-02,\n",
       "          1.5471e-01,  2.8976e-02,  1.5959e-01,  1.2961e-01, -2.0616e-02,\n",
       "          2.3263e-02, -7.3281e-02,  7.3923e-02,  1.7951e-02,  1.2365e-01,\n",
       "         -3.6842e-02, -6.3040e-02,  2.0126e-03, -9.2923e-02,  3.8697e-02,\n",
       "         -1.0958e-03, -5.2375e-02, -7.3116e-03, -6.7277e-02,  2.9157e-02,\n",
       "          2.6096e-02, -1.2770e-01,  3.6444e-02,  4.7034e-02, -8.5318e-02,\n",
       "          3.5449e-02,  2.4150e-02, -5.3790e-02, -3.6228e-02, -1.4897e-01,\n",
       "          9.2466e-03,  2.9003e-03, -4.9869e-02, -7.1029e-02,  1.8337e-02,\n",
       "          1.2111e-02, -8.0972e-03, -1.9230e-02,  4.0972e-03, -7.1587e-02,\n",
       "          1.0786e-01,  1.2824e-01, -6.0857e-03, -4.0126e-02, -2.2698e-02,\n",
       "         -5.7879e-02]], grad_fn=<SiluBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SinusoidalTimeEmbeddings(nn.Module):\n",
    "    def __init__(self, time_embed_dim, scaled_time_embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # This one is untrainable\n",
    "        self.inv_freq = nn.Parameter(\n",
    "            1.0\n",
    "            / (10000 ** (torch.arange(0, time_embed_dim, 2).float() / time_embed_dim)),\n",
    "            requires_grad=False,\n",
    "        )\n",
    "\n",
    "        # This one is trainable\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_embed_dim, scaled_time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(scaled_time_embed_dim, scaled_time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, timesteps: torch.Tensor):\n",
    "        timestep_freqs = timesteps.unsqueeze(1) * self.inv_freq.unsqueeze(0)\n",
    "        embeddings = torch.cat(\n",
    "            [torch.sin(timestep_freqs), torch.cos(timestep_freqs)], dim=-1\n",
    "        )\n",
    "        embeddings = self.time_mlp(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "s = SinusoidalTimeEmbeddings(time_embed_dim=128, scaled_time_embed_dim=256)\n",
    "timesteps = torch.tensor([1, 2, 3])\n",
    "s(timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a25b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
